<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->

- [容器中的网络](#%E5%AE%B9%E5%99%A8%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C)
  - [单机网络](#%E5%8D%95%E6%9C%BA%E7%BD%91%E7%BB%9C)
  - [跨主机网络](#%E8%B7%A8%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C)
  - [Flannel 的工作原理](#flannel-%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86)
  - [Flannel 的工作模式](#flannel-%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%A8%A1%E5%BC%8F)
    - [UDP](#udp)
    - [VXLAN](#vxlan)
    - [host-gw](#host-gw)
  - [参考](#%E5%8F%82%E8%80%83)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

## 容器中的网络

### 单机网络

docker 容器是一种特殊的进程，docker 容器在创建进程时，指定了这个进程所需要启用的一组 Namespace 参数。这样，容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。而对于宿主机以及其他不相关的程序，它就完全看不到了。   

容器可以直接使用宿主机的网络栈（–net=host），即：不开启 `Network Namespace`，比如：  

```
$ docker run -d --net=host --name nginx-host nginx
```

在这种情况下，这个容器启动后，直接监听的就是宿主机的 80 端口。  

通常情况下，容器一般会选择使用自己 `Network Namespace` 里的网络栈，即：拥有属于自己的 IP 地址和端口。这种情况下，被隔离的容器进程是如何和其他的容器进程进行通信呢？    

Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连接在 docker0 网桥上的容器，就可以通过它来进行通信。容器中通过 `Veth Pair` 来连接到 docker0 网桥上。   

什么是 `Veth Pair` 呢？  

`veth pair` 是成对出现的一种虚拟网络设备接口，一端连着网络协议栈，一端彼此相连。  

`Veth Pair` 的特点总是以两张虚拟网卡（Veth Peer）的形式成对出现的。从其中一个"网卡"发出的数据包，可以直接出现在与它对应的另一张"网卡"上，这两张网卡可以在不同的 `Network Namespace` 里。所以 `Veth Pair` 可以用来进行跨 `Network Namespace` 网络互联。   

先来启动两个容器   

```
$ docker run -d  --name nginx-1 nginx:alpine

$ docker run -d  --name nginx-2 nginx:alpine
```

查看宿主机的网络    

```
$ ifconfig
docker0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255
        inet6 fe80::42:f8ff:fe43:b13a  prefixlen 64  scopeid 0x20<link>
        ether 02:42:f8:43:b1:3a  txqueuelen 0  (Ethernet)
        RX packets 19  bytes 532 (532.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 25  bytes 1278 (1.2 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
...

veth228debd: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet6 fe80::10f4:65ff:fe6f:56d2  prefixlen 64  scopeid 0x20<link>
        ether 12:f4:65:6f:56:d2  txqueuelen 0  (Ethernet)
        RX packets 17  bytes 1554 (1.5 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 58  bytes 5156 (5.0 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

veth28bb8da: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet6 fe80::789f:75ff:fec5:5601  prefixlen 64  scopeid 0x20<link>
        ether 7a:9f:75:c5:56:01  txqueuelen 0  (Ethernet)
        RX packets 50  bytes 4508 (4.4 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 27  bytes 2286 (2.2 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
        
$ brctl show
bridge name	bridge id		STP enabled	interfaces
docker0		8000.000000000000	no		veth228debd
										veth28bb8da
```

可以看到 docker0 中被插入了 veth228debd 和 veth28bb8da 两个虚拟网卡，这个就是容器对应的 `Veth Pair` 设备，在宿主机上的虚拟网卡。连个容器，一个对应一个网卡。   

看下容器中的网络  

```
$ docker exec -it nginx-1 /bin/sh
$ ifconfig
eth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:02
          inet addr:172.17.0.2  Bcast:172.17.255.255  Mask:255.255.0.0
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:825 errors:0 dropped:0 overruns:0 frame:0
          TX packets:859 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:76962 (75.1 KiB)  TX bytes:79974 (78.0 KiB)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
```

容器中有一个 eth0 网卡，这个就是一个 `Veth Pair`，它的一端在容器的 `Network Namespace`，另一端位于宿主机上（`Host Namespace`），并且被“插”在了宿主机的 docker0 网桥上。  

这些虚拟网卡被插在网桥中，这些虚拟网卡调用网络协议栈处理数据包的能力，会被网桥接管，由网桥来处理。  

1、当容器1访问容器2的时候，首先容器1会通过 eth0 网卡发送一个 ARP 广播，来查找对方的 MAC 地址；    

2、当收到这些请求的时候，docker0 网桥就会扮演二层交换机的角色，会把 ARP 广播转发到其他被“插”在 docker0 上的虚拟网卡上；  

3、这样，同样连接在 docker0 上的容器2的网络协议栈就会收到这个 ARP 请求，从而将自己的 MAC 地址回复给容器1；  

4、有了对方的 MAC 地址，这样就能将请求发送出去了，  

5、这样多个容器通过 `Veth Pair` 打通自己和 docker0 网桥的网络，就能实现相互的网络通信。   

<img src="/img/k8s/k8s-vethPair.png"  alt="k8s" />    

所以被限制在 `Network Namespace` 里的容器进程，实际上是通过 `Veth Pair` 设备 + 宿主机网桥的方式，实现了跟同其他容器的数据交换。  

宿主机访问宿主机中的容器 IP，宿主机的数据包也是首先被发送到 docker0 网桥，然后通过绑定在网桥中的的 `Veth Pair` 设备，最终发送到容器中。     

两台宿主机中的容器通信见下文   

### 跨主机网络

单机的情况下使用(网桥模式)就是实现不同容器之间的 IP 互相访问，但是容器位于不同的宿主机中，在 Docker 的默认配置下就不能通过 IP 相互访问了。     

不同主机中的容器如何通信，这就涉及到跨主机网络访问的原理了，其中 Flannel 就是一种实现。    

Flannel 项目是 CoreOS 公司主推的容器网络方案。  

### Flannel 的工作原理

Flannel 实质上就是一种 Overlay 网络，也就是将 TCP 数据包装在另一种网络包里面进行路由转发和通信。   

Flannel 会在每一个宿主机上运行名为 flanneld 代理，主要实现 overlay 网络, 包含给所有 node 分配 subset 段等。Flannel 管理的容器网络中，一台宿主机上所有的容器，都属于该宿主机被分配的一个子网。不同的宿主机，是不同的子网段。       

这些子网与宿主机的对应关系，会保存在 Etcd 中。   

### Flannel 的工作模式

Flannel有下面三种后端实现：   

- UDP 模式：使用设备 flannel.0 进行封包解包，不是内核原生支持，频繁地内核态用户态切换，性能非常差；  

- VXLAN 模式：使用 flannel.1 进行封包解包，内核原生支持，性能较强；  

- host-gw 模式：无需 flannel.1 这样的中间设备，直接宿主机当作子网的下一跳地址，性能最强；   

#### UDP   

UDP 模式是 Flannel 项目最早支持的一种方式，也是性能最差的一种方式。这个模式目前已经被启用了。  

这个模式是最直接，最容易理解的容器跨主网络实现，所以这里先来看下 UDP 的具体的实现。    





#### VXLAN

#### host-gw











### 参考

【深入剖析 Kubernetes】https://time.geekbang.org/column/intro/100015201?code=UhApqgxa4VLIA591OKMTemuH1%2FWyLNNiHZ2CRYYdZzY%3D  
【循序渐进理解CNI机制与Flannel工作原理】https://blog.yingchi.io/posts/2020/8/k8s-flannel.html    



